# 数据分析工具包技术文档


## 一、概述

本工具包基于LangChain与大语言模型构建，提供一站式数据集分析能力，涵盖数据字典生成、算法规则构建、应用场景定位、采集情况分析及数据清洗等核心功能。通过自动化分析数据集样本（前20行数据），输出标准化的分析结果，支持多格式导出（PDF/Excel/Text），适用于数据治理、场景适配及预处理规划等场景。


## 二、核心模块说明

### 2.1 数据算法规则生成（data_algorithm.py）

#### 功能概述
基于数据集样本生成标准化的数据分析与处理规则体系，涵盖数据解构、预处理、特征工程及模型应用全流程，输出技术规范文档并支持PDF导出。

#### 核心组件
- **提示词模板（DATA_ANALYSIS_PROMPT）**：定义四层分析框架
  - 数据基础解构与特征识别（字段语义、场景适配、质量审计）
  - 预处理规则技术规范（清洗、转换协议）
  - 特征工程数学规则（派生特征、筛选机制）
  - 模型应用与行业适配（建模方向、跨领域通用性）

- **核心函数**
  ```python
  # 分析数据集并生成规则
  analyze_dataset(data_sample=None, dataframe=None, max_retries=3)
  
  # 保存分析结果为PDF（支持中文显示）
  save_to_pdf(text_content, dataset_name)
  ```

- **输入输出**
  - 输入：数据集样本（文本）或DataFrame
  - 输出：结构化技术规范文本（800-1000字符），PDF文件路径


### 2.2 数据字典生成（data_dictionary.py）

#### 功能概述
解析数据集字段属性，生成标准化数据字典，包含字段中英文名称、类型、长度、样例数据及主键标识，支持Excel格式导出。

#### 核心组件
- **提示词模板（DATA_ANALYSIS_PROMPT）**：约束输出JSON格式，包含6个字段属性：
  - 字段中文名（业务含义映射）
  - 字段英文名（驼峰/下划线命名法）
  - 字段类型（预定义12种类型，如INT/VARCHAR/DATE等）
  - 字段长度（基于类型特性与样本数据确定）
  - 样例数据（从样本中提取，非虚构）
  - 是否主键（唯一标识字段）

- **核心函数**
  ```python
  # 分析数据集并提取JSON格式数据字典
  analyze_dataset(dataset_name, data_sample=None, dataframe=None, max_retries=3)
  
  # 保存数据字典为格式化Excel
  save_to_excel(json_data, dataset_name)
  ```

- **输入输出**
  - 输入：数据集名称、样本数据或DataFrame
  - 输出：JSON格式数据字典，Excel文件路径


### 2.3 数据应用场景分析（data_application.py）

#### 功能概述
基于数据集特征推断适配行业与应用场景，分析跨行业迁移性，输出结构化场景分析报告。

#### 核心组件
- **提示词模板（DATA_ANALYSIS_PROMPT）**：按五部分结构化输出
  - 行业属性定位（核心/部分适配场景及依据）
  - 场景核心定义（行业+具体场景描述）
  - 场景需求拆解（效率/风险/成本需求映射）
  - 数据支撑逻辑（字段组合与场景的协同关系）
  - 跨行业适配边界（可迁移特征与限制条件）

- **核心函数**
  ```python
  # 分析数据集应用场景
  analyze_dataset(dataset_name, data_sample=None, dataframe=None, max_retries=3)
  
  # 保存分析结果为文本文件
  save_to_text(text_content, dataset_name)
  ```

- **输入输出**
  - 输入：数据集名称、样本数据或DataFrame
  - 输出：场景分析文本，TXT文件路径


### 2.4 数据采集情况分析（data_collection.py）

#### 功能概述
区分数据集中的内部/外部数据，分析其采集、处理及存储方式，输出通用化采集流程说明。

#### 核心组件
- **提示词模板（DATA_ANALYSIS_PROMPT）**：严格分为两部分输出
  - 内部数据：组织内部生成的数据（采集方式、处理逻辑、存储机制）
  - 外部数据：第三方/公开数据源（获取渠道、解析方法、整合策略）

- **核心函数**
  ```python
  # 分析数据采集情况
  analyze_dataset(dataset_name, data_sample=None, dataframe=None, max_retries=3)
  
  # 保存分析结果为文本文件
  save_to_text(text_content, dataset_name)
  ```

- **输入输出**
  - 输入：数据集名称、样本数据或DataFrame
  - 输出：内部/外部数据采集分析文本，TXT文件路径


### 2.5 数据清洗工具（data_cleaning.py）

#### 功能概述
提供数据清洗自动化能力，支持重复值处理、缺失值填充、异常值移除、文本标准化等操作，支持Excel与PDF格式输出清洗结果。

#### 核心组件
- **数据清洗操作**
  - 重复值处理（基于指定字段去重）
  - 缺失值填充（均值/中位数/模式/前后向填充）
  - 异常值移除（IQR方法）
  - 文本标准化（大小写转换、去标点等）

- **格式转换功能**
  - 临时数据库创建（CSV/Excel转SQLite）
  - Excel格式化（自动列宽、边框、文本换行）
  - Excel转PDF（支持Windows COM组件与FPDF两种方式）

- **核心函数**
  ```python
  # 创建临时数据库并导入数据
  create_temp_sql_db(file_path)
  
  # 执行数据清洗操作
  apply_pandas_cleaning(df, cleaning_operations)
  
  # Excel转PDF
  _excel_to_pdf(excel_path, pdf_path, max_rows=300)
  ```


## 三、技术架构

1. **依赖框架**
   - 大语言模型交互：LangChain（PromptTemplate/ChatOpenAI/StrOutputParser）
   - 数据处理：pandas
   - 格式导出：fpdf（PDF）、openpyxl（Excel）、sqlite3（临时数据库）

2. **核心流程**
   ```
   数据输入（CSV/Excel）→ 样本提取（前20行）→ 大模型分析（按模块提示词）→ 结果结构化 → 多格式导出
   ```

3. **配置项**
   - 模型配置：`MODEL_NAME`（大语言模型名称）
   - 路径配置：`DEFAULT_OUTPUT_DIR`（默认输出目录）
   - 中文支持：内置字体检测机制（优先simhei.ttf、msyh.ttf等）


## 四、使用示例

以“定日镜场”数据集为例，工具包输出如下：

1. **应用场景分析结果**（部分）：
   ```
   行业属性定位
   核心适配场景为能源行业太阳能光热发电，依据是定日镜序号与空间坐标构成设备唯一标识，尺寸参数对应光学性能指标...
   
   场景核心定义
   能源行业定日镜场光学效率优化场景。通过镜面空间坐标与几何尺寸的关联分析，实现反射光斑聚焦精度提升...
   ```

2. **数据字典示例**（JSON片段）：
   ```json
   {
     "algorithm_rules": [
       {"字段中文名": "定日镜序号", "字段英文名": "heliostat_id", "字段类型": "INT", "字段长度": "11", "样例数据": "101", "是否主键": "是"},
       {"字段中文名": "X坐标", "字段英文名": "x_coordinate", "字段类型": "FLOAT", "字段长度": "12", "样例数据": "235.67", "是否主键": "否"}
     ]
   }
   ```


## 五、注意事项

1. 中文支持：确保系统安装中文字体（如simhei.ttf），否则可能导致PDF/Excel中文显示异常
2. 模型依赖：需配置有效的OpenAI兼容API密钥与基础地址
3. 数据样本：默认使用前20行数据作为样本，确保该范围包含足够的字段特征
4. 错误处理：各分析函数均包含重试机制（默认3次），应对临时网络或模型响应异常